{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for free speech feature extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from statistics import mean, stdev\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from scipy import spatial\n",
    "from scipy import stats\n",
    "import string\n",
    "import math\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "from scipy.stats import linregress\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# only if it has not been downloaded, uncomment:\n",
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data \n",
    "# a file called \"free_speech.xlsx\"\n",
    "\n",
    "# COLUMNS: \n",
    "# - FILE (a unique designator), \n",
    "# - TRANSCRIPT (human transcribed transcript)\n",
    "# - GROUP (1 = control, 2 = aMCI, 3 = AD)\n",
    "\n",
    "free_speech = pd.read_excel('free_speech.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "free_speech['participant_wc'] = free_speech.apply(lambda row: len(row['TRANSCRIPT'].split()), axis=1)\n",
    "free_speech['participant_types'] = free_speech.apply(lambda row: len(set(row['TRANSCRIPT'].split())), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_token_ratio = []\n",
    "\n",
    "for index, row in free_speech.iterrows():\n",
    "    type_token_ratio.append(row['participant_types']/row['participant_wc'])\n",
    "    \n",
    "free_speech['participant_type_token_ratio'] = type_token_ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brunets_index = []\n",
    "\n",
    "for index, row in free_speech.iterrows():\n",
    "    # log(wc**types**-0.165) = (types**-0.165)*log(wc)\n",
    "    brunets_index.append(row['participant_types']**(-0.165)*math.log(row['participant_wc']))\n",
    "\n",
    "free_speech['participant_brunets_index'] = brunets_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment of all sentences per participant\n",
    "\n",
    "participant_mean_sentiment = []\n",
    "participant_max_sentiment = []\n",
    "participant_min_sentiment = []\n",
    "participant_stdv_sentiment = []\n",
    "\n",
    "for transcript in free_speech.TRANSCRIPT:\n",
    "    blob = TextBlob(transcript)\n",
    "    all_sentiments = [sentence.sentiment.polarity for sentence in blob.sentences]\n",
    "    participant_mean_sentiment.append(mean(all_sentiments))\n",
    "    participant_max_sentiment.append(max(all_sentiments))\n",
    "    participant_min_sentiment.append(min(all_sentiments))\n",
    "    participant_stdv_sentiment.append(stdev(all_sentiments))\n",
    "    \n",
    "free_speech['participant_mean_sentiment'] = participant_mean_sentiment\n",
    "free_speech['participant_max_sentiment'] = participant_max_sentiment\n",
    "free_speech['participant_min_sentiment'] = participant_min_sentiment\n",
    "free_speech['participant_stdv_sentiment'] = participant_stdv_sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parts of speech frequencies per participant\n",
    "\n",
    "participant_noun_freq = []\n",
    "participant_determiner_freq = []\n",
    "participant_preposition_freq = []\n",
    "participant_base_verb_freq = []\n",
    "participant_pasttense_verb_freq = []\n",
    "participant_gerund_presentparticiple_verb_freq = []\n",
    "participant_pastparticiple_verb_freq = []\n",
    "participant_non3rdpersonsingularpresent_verb_freq = []\n",
    "participant_3rdpersonsingularpresent_verb_freq = []\n",
    "participant_TOTAL_verb_freq = []\n",
    "participant_to_freq = []\n",
    "participant_adverb_freq = []\n",
    "participant_adjective_freq = []\n",
    "participant_modal_freq = []\n",
    "participant_coordinating_conjunctions_freq = []\n",
    "participant_cardinals_freq = []\n",
    "participant_particle_freq = []\n",
    "participant_personal_pronoun_freq = []\n",
    "participant_wh_adverbs_freq = []\n",
    "participant_possessive_pronoun_freq = []\n",
    "participant_wh_determiner_freq = []\n",
    "participant_predeterminer_freq = []\n",
    "participant_interjection_freq = []\n",
    "participant_existential_there_freq = []\n",
    "participant_wh_pronoun_freq = []\n",
    "participant_content_density = []\n",
    "\n",
    "for transcript, wc in zip(free_speech.TRANSCRIPT, free_speech.participant_wc):\n",
    "    \n",
    "    blob = TextBlob(transcript)\n",
    "    \n",
    "    nouns = 0\n",
    "    determiners = 0\n",
    "    prepositions = 0\n",
    "    base_verbs = 0\n",
    "    pasttense_verbs = 0\n",
    "    verb_gerund_presentparticiple = 0\n",
    "    verb_pastparticiple = 0\n",
    "    verb_non3rdpersonsingularpresent = 0\n",
    "    verb_3rdpersonsingularpresent = 0\n",
    "    tos = 0\n",
    "    adverbs = 0\n",
    "    adjectives = 0\n",
    "    modals = 0\n",
    "    coordinating_conjunctions = 0\n",
    "    cardinals = 0\n",
    "    particles = 0\n",
    "    personal_pronouns = 0\n",
    "    wh_adverbs = 0\n",
    "    possessive_pronouns = 0\n",
    "    wh_determiners = 0\n",
    "    predeterminers = 0\n",
    "    interjections = 0\n",
    "    existential_theres = 0\n",
    "    wh_pronouns = 0\n",
    "    \n",
    "    for word, tag in blob.tags:\n",
    "        \n",
    "        #all nouns grouped together: singular, plural, proper singular, proper plural \n",
    "        if tag == 'NN' or tag == 'NNS' or tag == 'NNP' or tag == 'NNPS':\n",
    "            nouns += 1\n",
    "        elif tag == 'DT':\n",
    "            determiners += 1\n",
    "        elif tag == 'IN':\n",
    "            prepositions += 1\n",
    "        elif tag == 'VB':\n",
    "            base_verbs +=1\n",
    "        elif tag == 'VBD':\n",
    "            pasttense_verbs += 1\n",
    "        elif tag == 'VBG':\n",
    "            verb_gerund_presentparticiple += 1\n",
    "        elif tag == 'VBN':\n",
    "            verb_pastparticiple += 1\n",
    "        elif tag == 'VBP':\n",
    "            verb_non3rdpersonsingularpresent += 1\n",
    "        elif tag == 'VBZ':\n",
    "            verb_3rdpersonsingularpresent += 1\n",
    "        elif tag == 'TO':\n",
    "            tos += 1\n",
    "        \n",
    "        #all adverbs grouped together: normal, comparative, superlative\n",
    "        elif tag == 'RB' or tag == 'RBR' or tag == 'RBS':\n",
    "            adverbs += 1\n",
    "        \n",
    "        #all adjectives grouped together: normal, comparative, superlative\n",
    "        elif tag == 'JJ' or tag == 'JJR' or tag == 'JJS':\n",
    "            adjectives += 1\n",
    "        elif tag == 'MD':\n",
    "            modals += 1\n",
    "        elif tag == 'CC':\n",
    "            coordinating_conjunctions += 1\n",
    "        elif tag == 'RP':\n",
    "            particles += 1\n",
    "        elif tag == 'CD':\n",
    "            cardinals += 1\n",
    "        elif tag == 'PRP':\n",
    "            personal_pronouns += 1\n",
    "        \n",
    "        #when\n",
    "        elif tag == 'WRB':\n",
    "            wh_adverbs += 1\n",
    "        elif tag == 'PRP$':\n",
    "            possessive_pronouns += 1\n",
    "        \n",
    "        #that\n",
    "        elif tag == 'WDT':\n",
    "            wh_determiners += 1\n",
    "        elif tag == 'PDT':\n",
    "            predeterminers += 1\n",
    "        elif tag == 'UH':\n",
    "            interjections += 1\n",
    "        elif tag == 'EX':\n",
    "            existential_theres += 1\n",
    "        \n",
    "        #who, what, whose\n",
    "        elif tag == 'WP' or tag == 'WP$':\n",
    "            wh_pronouns += 1\n",
    "            \n",
    "    total_verbs = base_verbs+pasttense_verbs+verb_gerund_presentparticiple+verb_pastparticiple+verb_non3rdpersonsingularpresent+verb_3rdpersonsingularpresent\n",
    "\n",
    "    participant_noun_freq.append(nouns/wc)\n",
    "    participant_determiner_freq.append(determiners/wc)\n",
    "    participant_preposition_freq.append(prepositions/wc)\n",
    "    participant_base_verb_freq.append(base_verbs/wc)\n",
    "    participant_pasttense_verb_freq.append(pasttense_verbs/wc)\n",
    "    participant_gerund_presentparticiple_verb_freq.append(verb_gerund_presentparticiple/wc)\n",
    "    participant_pastparticiple_verb_freq.append(verb_pastparticiple/wc)\n",
    "    participant_non3rdpersonsingularpresent_verb_freq.append(verb_non3rdpersonsingularpresent/wc)\n",
    "    participant_3rdpersonsingularpresent_verb_freq.append(verb_3rdpersonsingularpresent/wc)\n",
    "    participant_TOTAL_verb_freq.append(total_verbs/wc)\n",
    "    participant_to_freq.append(tos/wc)\n",
    "    participant_adverb_freq.append(adverbs/wc)\n",
    "    participant_adjective_freq.append(adjectives/wc)\n",
    "    participant_modal_freq.append(modals/wc)\n",
    "    participant_coordinating_conjunctions_freq.append(coordinating_conjunctions/wc)\n",
    "    participant_cardinals_freq.append(cardinals/wc)\n",
    "    participant_particle_freq.append(particles/wc)\n",
    "    participant_personal_pronoun_freq.append(personal_pronouns/wc)\n",
    "    participant_wh_adverbs_freq.append(wh_adverbs/wc)\n",
    "    participant_possessive_pronoun_freq.append(possessive_pronouns/wc)\n",
    "    participant_wh_determiner_freq.append(wh_determiners/wc)\n",
    "    participant_predeterminer_freq.append(predeterminers/wc)\n",
    "    participant_interjection_freq.append(interjections/wc)\n",
    "    participant_existential_there_freq.append(existential_theres/wc)\n",
    "    participant_wh_pronoun_freq.append(wh_pronouns/wc)\n",
    "    participant_content_density.append((total_verbs+nouns+adjectives+adverbs)/wc)\n",
    "    \n",
    "    \n",
    "free_speech['participant_noun_freq'] = participant_noun_freq\n",
    "free_speech['participant_determiner_freq'] = participant_determiner_freq\n",
    "free_speech['participant_preposition_freq'] = participant_preposition_freq\n",
    "free_speech['participant_base_verb_freq'] = participant_base_verb_freq\n",
    "free_speech['participant_pasttense_verb_freq'] = participant_pasttense_verb_freq\n",
    "free_speech['participant_gerund_presentparticiple_verb_freq'] = participant_gerund_presentparticiple_verb_freq\n",
    "free_speech['participant_pastparticiple_verb_freq'] = participant_pastparticiple_verb_freq\n",
    "free_speech['participant_non3rdpersonsingularpresent_verb_freq'] = participant_non3rdpersonsingularpresent_verb_freq\n",
    "free_speech['participant_3rdpersonsingularpresent_verb_freq'] = participant_3rdpersonsingularpresent_verb_freq\n",
    "free_speech['participant_TOTAL_verb_freq'] = participant_TOTAL_verb_freq\n",
    "free_speech['participant_to_freq'] = participant_to_freq\n",
    "free_speech['participant_adverb_freq'] = participant_adverb_freq\n",
    "free_speech['participant_adjective_freq'] = participant_adjective_freq\n",
    "free_speech['participant_modal_freq'] = participant_modal_freq\n",
    "free_speech['participant_coordinating_conjunctions_freq'] = participant_coordinating_conjunctions_freq\n",
    "free_speech['participant_cardinals_freq'] = participant_cardinals_freq\n",
    "free_speech['participant_particle_freq'] = participant_particle_freq\n",
    "free_speech['participant_personal_pronoun_freq'] = participant_personal_pronoun_freq\n",
    "free_speech['participant_wh_adverbs_freq'] = participant_wh_adverbs_freq\n",
    "free_speech['participant_possessive_pronoun_freq'] = participant_possessive_pronoun_freq\n",
    "free_speech['participant_wh_determiner_freq'] = participant_wh_determiner_freq\n",
    "free_speech['participant_predeterminer_freq'] = participant_predeterminer_freq\n",
    "free_speech['participant_interjection_freq'] = participant_interjection_freq\n",
    "free_speech['participant_existential_there_freq'] = participant_existential_there_freq\n",
    "free_speech['participant_wh_pronoun_freq'] = participant_wh_pronoun_freq\n",
    "free_speech['participant_content_density'] = participant_content_density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COHERENCE FUNCTIONS\n",
    "\n",
    "# load embeddings (can be swapped for various types) - These are the \"don't count, predict!\" embeddings: https://zenodo.org/record/2635544#.YqpI7BPMJQI\n",
    "embeddings = {}\n",
    "input_file = open('EN-wform.w.5.cbow.neg10.400.subsmpl.txt', 'r') \n",
    "\n",
    "for line in input_file:\n",
    "    tokens = line.split('\\t')\n",
    "    tokens[-1] = tokens[-1].strip()\n",
    "    for i in range(1, len(tokens)):\n",
    "        tokens[i] = float(tokens[i])\n",
    "    embeddings[tokens[0]] = tokens[1:-1]\n",
    "\n",
    "\n",
    "# GOOGLE NEWS W2V: \n",
    "# w2v = models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "def vector_sum(vectors):\n",
    "    '''\n",
    "    given a list of vectors for a sentence, return the sum of all vectors\n",
    "    '''\n",
    "    n = len(vectors)\n",
    "    d = len(vectors[0])\n",
    "\n",
    "    #create an array initialized to 0 of the same length of the word embeddings\n",
    "    s = []\n",
    "    for i in range(d):\n",
    "        s.append(0)\n",
    "    s = np.array(s)\n",
    "\n",
    "    #add each word embedding to the zero vector\n",
    "    for vector in vectors:\n",
    "        s = s + np.array(vector)\n",
    "\n",
    "    return list(s)\n",
    "\n",
    "def get_cosine(text1, text2):\n",
    "    text1_embeddings = []\n",
    "    text2_embeddings = []\n",
    "    \n",
    "    for word in text1.split():\n",
    "        lemma = str(word).translate(str.maketrans(\"\", \"\", string.punctuation)).lower()\n",
    "        if lemma in embeddings:\n",
    "            text1_embeddings.append(embeddings[lemma]) # w2v[lemma] if using the google news vectors\n",
    "        else:\n",
    "            print('this word has no embedding: ', lemma)\n",
    "            print(text1)\n",
    "            \n",
    "    for word in text2.split():\n",
    "        lemma = str(word).translate(str.maketrans(\"\", \"\", string.punctuation)).lower()\n",
    "        if lemma in embeddings:\n",
    "            text2_embeddings.append(embeddings[lemma])\n",
    "        else:\n",
    "            print('this word has no embedding: ', lemma)\n",
    "            print(text2)\n",
    "            \n",
    "    text1_sum = vector_sum(text1_embeddings)\n",
    "    text2_sum = vector_sum(text2_embeddings)\n",
    "    \n",
    "    cos = 1 - spatial.distance.cosine(text1_sum, text2_sum)\n",
    "    \n",
    "    return cos\n",
    "\n",
    "def get_ngrams_len(text, n):\n",
    "    '''\n",
    "    doing this a complicated way but could just do len(raudio.split()) - 3\n",
    "    '''\n",
    "    n_grams = ngrams(word_tokenize(text), n)\n",
    "    ngrams_list = [' '.join(grams) for grams in n_grams]\n",
    "    return len(ngrams_list)\n",
    "\n",
    "\n",
    "def get_ngrams(text, n):\n",
    "    n_grams = ngrams(word_tokenize(text), n)\n",
    "    return [' '.join(grams) for grams in n_grams]\n",
    "\n",
    "\n",
    "def get_slope(nums):\n",
    "\n",
    "    x = range(len(nums))\n",
    "    y = nums\n",
    "\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
    "\n",
    "    return slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat with different ngram lengths -> a range of [3 - 8] \n",
    "\n",
    "participant_overall_mean_coherence_4 = []\n",
    "participant_overall_std_coherence_4 = []\n",
    "participant_overall_min_coherence_4 = []\n",
    "participant_overall_max_coherence_4 = []\n",
    "participant_overall_slope_coherence_4 = []\n",
    "\n",
    "for transcript in free_speech.TRANSCRIPT:\n",
    "    \n",
    "    cosines = []\n",
    "    \n",
    "    ## CAN OPTIONALLY REMOVE STOP WORDS FIRST ##\n",
    "    \n",
    "    fourgrams = get_ngrams(all_utterances, 4)\n",
    "    \n",
    "    for i in range(len(fourgrams)-1):\n",
    "        cosines.append(get_cosine(fourgrams[i], fourgrams[i+1]))\n",
    "    \n",
    "    participant_overall_mean_coherence_4.append(np.array(cosines).mean())\n",
    "    participant_overall_std_coherence_4.append(np.array(cosines).std())\n",
    "    participant_overall_min_coherence_4.append(min(cosines))\n",
    "    participant_overall_min_coherence_4.append(max(cosines))\n",
    "    participant_overall_slope_coherence_4.append(get_slope(cosines))\n",
    "    \n",
    "free_speech['participant_DCP_mean_coherence_4'] = participant_overall_mean_coherence_4\n",
    "free_speech['participant_DCP_std_coherence_4'] = participant_overall_std_coherence_4\n",
    "free_speech['participant_DCP_min_coherence_4'] = participant_overall_min_coherence_4\n",
    "free_speech['participant_DCP_max_coherence_4'] = participant_overall_max_coherence_4\n",
    "free_speech['participant_DCP_slope_coherence_4'] = participant_overall_slope_coherence_4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coherence between interviewer MAIN question and interviewee responses\n",
    "\n",
    "main_question = 'Can you think of a fun or particularly memorable event from when you were a child that you can tell me about?'\n",
    "\n",
    "slopes = []\n",
    "avgs = []\n",
    "stds = []\n",
    "\n",
    "for transcript in free_speech.TRANSCRIPT:\n",
    "    \n",
    "    cosines = []\n",
    "    fourgrams = get_ngrams(transcript, 4)\n",
    "    for i in range(len(fourgrams)):\n",
    "        cosines.append(get_cosine(fourgrams[i], main_question))\n",
    "    \n",
    "    avgs.append(np.array(cosines).mean())\n",
    "    stds.append(np.array(cosines).std())\n",
    "    \n",
    "    slope, _, _, _, std_err = stats.linregress(range(len(cosines)),cosines)\n",
    "    \n",
    "    slopes.append(slope)\n",
    "    \n",
    "free_speech['participant_coherence_to_Q_slope'] = slopes\n",
    "free_speech['participant_coherence_to_Q_avg'] = avgs\n",
    "free_speech['participant_coherence_to_Q_std'] = stds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_cosine_BERT(text1, text2):\n",
    "\n",
    "    tokenized_text1 = tokenizer.encode(text1)\n",
    "    tokenized_text2 = tokenizer.encode(text2)\n",
    "\n",
    "    #convert indexed tokens in a PyTorch tensor\n",
    "    input_ids1 = torch.tensor(tokenized_text1).unsqueeze(0)\n",
    "    input_ids2 = torch.tensor(tokenized_text2).unsqueeze(0)\n",
    "  \n",
    "    #run the input tensor through the BertModel\n",
    "    #see text in above cell for what is contained in outputs variable\n",
    "    outputs1 = model(input_ids1)\n",
    "    outputs2 = model(input_ids2)\n",
    "\n",
    "    #get the last_hidden_state\n",
    "    last_hidden_state1 = outputs1[0]\n",
    "    last_hidden_state2 = outputs2[0]\n",
    "\n",
    "    #last hidden state is dimension (batch_size, sequence_length, hidden_size)\n",
    "    #we have one batch so grab this single batch - this_batch is a tensor for each token in tokenized_text\n",
    "    this_batch1 = last_hidden_state1[0]\n",
    "    this_batch2 = last_hidden_state2[0]\n",
    "  \n",
    "    #now get the 768 dimension vector for the CLS token (the first in the list) \n",
    "    cls_vector1 = this_batch1[0]\n",
    "    cls_vector2 = this_batch2[0]\n",
    "    \n",
    "    cos = 1 - spatial.distance.cosine(cls_vector1.detach().numpy(), cls_vector2.detach().numpy())\n",
    "    \n",
    "    return cos\n",
    "\n",
    "# again, redo this with different ngram sizes in [3-8]\n",
    "\n",
    "participant_BERT_mean_coherence_4 = []\n",
    "participant_BERT_std_coherence_4 = []\n",
    "participant_BERT_min_coherence_4 = []\n",
    "participant_BERT_max_coherence_4 = []\n",
    "participant_BERT_slope_coherence_4 = []\n",
    "\n",
    "for transcript in free_speech.TRANSCRIPT:\n",
    "    cosines = []\n",
    "    fourgrams = get_ngrams(transcript, 4)\n",
    "    for i in range(len(fourgrams)-1):\n",
    "        cosines.append(get_cosine_BERT(fourgrams[i], fourgrams[i+1]))\n",
    "\n",
    "    participant_BERT_mean_coherence_4.append((np.array(cosines).mean())\n",
    "    participant_BERT_std_coherence_4.append(np.array(cosines).std())\n",
    "    participant_BERT_min_coherence_4.append(min(cosines))\n",
    "    participant_BERT_max_coherence_4.append(max(cosines))\n",
    "    participant_BERT_slope_coherence_4.append(get_slope(cosines))\n",
    "\n",
    "free_speech['participant_BERT_mean_coherence_4'] = participant_BERT_mean_coherence_4\n",
    "free_speech['participant_BERT_std_coherence_4'] = participant_BERT_std_coherence_4\n",
    "free_speech['participant_BERT_min_coherence_4'] = participant_BERT_min_coherence_4\n",
    "free_speech['participant_BERT_max_coherence_4'] = participant_BERT_max_coherence_4\n",
    "free_speech['participant_BERT_slope_coherence_4'] = participant_BERT_slope_coherence_4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we want to parse each sentence and get the max parse depth of each\n",
    "\n",
    "# first download stanford corenlp https://stanfordnlp.github.io/CoreNLP/download.html\n",
    "# then run the server inside the folder:\n",
    "\n",
    "# export CLASSPATH=\"`find . -name '*.jar'`\"\n",
    "# java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer [port?] \n",
    "\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "\n",
    "all_max_depths = []\n",
    "#for each utterance\n",
    "for utterance in free_speech.TRANSCRIPT:\n",
    "    \n",
    "    sentences = sent_tokenize(utterance)\n",
    "    \n",
    "    # for sentence in each utterance\n",
    "    max_depths = []\n",
    "    for text in sentences:\n",
    "        bracket_counter = []\n",
    "        \n",
    "        #get the parse\n",
    "        output = nlp.annotate(text, properties={\n",
    "            'annotators': 'parse',\n",
    "            'outputFormat': 'json'\n",
    "        })\n",
    "        \n",
    "        try:\n",
    "            parse = output['sentences'][0]['parse'].split('\\n')\n",
    "        except:\n",
    "            all_max_depths.append('sentence too long')\n",
    "            continue\n",
    "            \n",
    "        for line in parse:\n",
    "            bracket_counter.append(line.count(')'))\n",
    "            \n",
    "        #get the furthest depth of this setence\n",
    "        max_depths.append(max(bracket_counter)-1)\n",
    "    \n",
    "    #record max depths of all sentences in an utterance\n",
    "    all_max_depths.append(max_depths)\n",
    "    \n",
    "# list of maximum depth per sentence in utterance (not used as a feature)\n",
    "free_speech['participant_parse_depth_per_sentence'] = all_max_depths\n",
    "\n",
    "participant_parse_max_depth = []\n",
    "participant_parse_min_depth = []\n",
    "participant_parse_mean_depth = []\n",
    "participant_parse_std_depth = []\n",
    "\n",
    "for listofdepths in all_max_depths:\n",
    "    try:\n",
    "        listofdepths.remove('sentence too long')\n",
    "    except:\n",
    "        pass\n",
    "    participant_parse_max_depth.append(max(listofdepths))\n",
    "    participant_parse_min_depth.append(min(listofdepths))\n",
    "    participant_parse_mean_depth.append(np.array(listofdepths).mean())\n",
    "    participant_parse_std_depth.append(np.array(listofdepths).std())\n",
    "    \n",
    "free_speech['participant_parse_max_depth'] = participant_parse_max_depth\n",
    "free_speech['participant_parse_min_depth'] = participant_parse_min_depth\n",
    "free_speech['participant_parse_mean_depth'] = participant_parse_mean_depth\n",
    "free_speech['participant_parse_std_depth'] = participant_parse_std_depth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_ums_or_ahs = []\n",
    "participant_ums_or_ahs_freq = []\n",
    "\n",
    "for utterance, wc in zip(free_speech.TRANSCRIPT, free_speech.participant_wc):\n",
    "    total_ums_ahs = 0\n",
    "    for word in utterance.split():\n",
    "        if word.lower() == 'um' or word.lower() == 'ah' or word.lower() == 'uh':\n",
    "            total_ums_ahs += 1\n",
    "\n",
    "    participant_ums_or_ahs.append(total_ums_ahs)\n",
    "    participant_ums_or_ahs_freq.append(total_ums_ahs/wc)\n",
    "    \n",
    "free_speech['participant_ums_or_ahs_count'] = participant_ums_or_ahs\n",
    "free_speech['participant_ums_or_ahs_freq'] = participant_ums_or_ahs_freq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "free_speech.to_excel('free_speech_features.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the f statistics for individual features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in free_speech.columns:\n",
    "    if column != 'FILE' and column != 'TRANSCRIPT' and column != 'GROUP' and column != 'participant_parse_depth_per_sentence':\n",
    "        \n",
    "        ones = free_speech[free_speech.GROUP==1]\n",
    "        twos = free_speech[free_speech.GROUP==2]\n",
    "        threes = free_speech[free_speech.GROUP==3]\n",
    "        twosthrees = free_speech[(free_speech.GROUP==2)|(free_speech.GROUP==3)]\n",
    "        \n",
    "        f1, p1 = stats.f_oneway(ones[column].dropna(), twos[column].dropna(), threes[column].dropna())\n",
    "        f2, p2 = stats.f_oneway(ones[column].dropna(), twos[column].dropna())\n",
    "        f3, p3 = stats.f_oneway(ones[column].dropna(), threes[column].dropna())\n",
    "        f4, p4 = stats.f_oneway(twos[column].dropna(), threes[column].dropna())\n",
    "        f5, p5 = stats.f_oneway(ones[column].dropna(), twosthrees[column].dropna())\n",
    "\n",
    "        print(column, 'overall', (f1,p1), '1vs2', (f2,p2),'1vs3', (f3,p3),'2vs3', (f4,p4), '1vs23', (f5,p5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
