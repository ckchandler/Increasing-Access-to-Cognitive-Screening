{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for ML grid search + cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "from numpy import set_printoptions\n",
    "set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first load in the data\n",
    "\n",
    "# - FILE (a unique designator), \n",
    "# - GROUP (1 = control, 2 = aMCI, 3 = AD)\n",
    "# - ANIMALS / TRANSCRIPT (the human transcription of animal fluency / free speech)\n",
    "# - <features>\n",
    "\n",
    "af = pd.read_excel('animal_fluency_features.xlsx')\n",
    "fs = pd.read_excel('free_speech_features.xlsx')\n",
    "\n",
    "# merge the two feature sets on the FILE column\n",
    "df = af.merge(fs, on='FILE')\n",
    "\n",
    "# drop the columns you don't want to use for feature analysis\n",
    "df = df.drop(columns=['participant_parse_depth_per_sentence'])\n",
    "df = df.drop(columns=['TRANSCRIPT'])\n",
    "df = df.drop(columns=['ANIMALS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top features based on ANOVA F value\n",
    "\n",
    "dfarray = df.values\n",
    "X = dfarray[:,1:-1] # first column is FILE, last is GROUP \n",
    "Y = dfarray[:,-1] # last is GROUP\n",
    "\n",
    "# feature extraction\n",
    "test = SelectKBest(score_func=f_classif, k=10)\n",
    "fit = test.fit(X, Y)\n",
    "\n",
    "# summarize scores\n",
    "scores = fit.scores_\n",
    "\n",
    "# Indices of 10 largest elements in list \n",
    "# using sorted() + lambda + list slicing \n",
    "res = sorted(range(len(scores)), key = lambda sub: scores[sub], reverse=True)[:10] \n",
    "  \n",
    "# printing top 10 f value featueres \n",
    "print(\"Indices list of max 10 elements is : \" + str(res)) \n",
    "for i, ind in zip(range(len(res)), res):\n",
    "    print(str(i), ':', df.columns[1:-1][ind])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top features with recursive feature elimination \n",
    "\n",
    "model = LogisticRegression()\n",
    "rfe = RFE(model, 10)\n",
    "fit = rfe.fit(X, Y) # from above\n",
    "\n",
    "print(\"Num Features: %d\" % fit.n_features_)\n",
    "print(\"Selected Features: %s\" % fit.support_)\n",
    "print(\"Feature Ranking: %s\" % fit.ranking_)\n",
    "\n",
    "chosenfeatures = [df.columns[1:-1][i] for i in range(len(fit.support_)) if fit.support_[i]]\n",
    "print('chosen features: ', chosenfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top features from feature importance in extra trees classifier\n",
    "\n",
    "model = ExtraTreesClassifier(n_estimators=10)\n",
    "model.fit(X, Y)\n",
    "\n",
    "scores = model.feature_importances_\n",
    "\n",
    "# Indices of 10 largest elements in list \n",
    "# using sorted() + lambda + list slicing \n",
    "res = sorted(range(len(scores)), key = lambda sub: scores[sub], reverse=True)[:10] \n",
    "  \n",
    "# printing top 10 f value featueres \n",
    "print(\"Indices list of max 10 elements is : \" + str(res)) \n",
    "for i, ind in zip(range(len(res)), res):\n",
    "    print(str(i), ':', df.columns[1:-1][ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heat map of correlations to group label\n",
    "\n",
    "corr = df.corr()\n",
    "\n",
    "corr.style.background_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EstimatorSelectionHelper:\n",
    "\n",
    "    def __init__(self, models, params):\n",
    "        if not set(models.keys()).issubset(set(params.keys())):\n",
    "            missing_params = list(set(models.keys()) - set(params.keys()))\n",
    "            raise ValueError(\"Some estimators are missing parameters: %s\" % missing_params)\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.keys = models.keys()\n",
    "        self.grid_searches = {}\n",
    "\n",
    "    def fit(self, X, y, cv=3, n_jobs=3, verbose=1, scoring=None, refit=False):\n",
    "        for key in self.keys:\n",
    "            print(\"Running GridSearchCV for %s.\" % key)\n",
    "            model = self.models[key]\n",
    "            params = self.params[key]\n",
    "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs,\n",
    "                              verbose=verbose, scoring=scoring, refit=refit,\n",
    "                              return_train_score=True)\n",
    "            gs.fit(X,y)\n",
    "            self.grid_searches[key] = gs    \n",
    "\n",
    "    def score_summary(self, sort_by='mean_score'):\n",
    "        def row(key, scores, params):\n",
    "            d = {\n",
    "                 'estimator': key,\n",
    "                 'min_score': min(scores),\n",
    "                 'max_score': max(scores),\n",
    "                 'mean_score': np.mean(scores),\n",
    "                 'std_score': np.std(scores),\n",
    "            }\n",
    "            return pd.Series({**params,**d})\n",
    "\n",
    "        rows = []\n",
    "        for k in self.grid_searches:\n",
    "            print(k)\n",
    "            params = self.grid_searches[k].cv_results_['params']\n",
    "            scores = []\n",
    "            for i in range(92): # if doing k folds: for i in range(self.grid_searches[k].cv): \n",
    "                key = \"split{}_test_score\".format(i)\n",
    "                r = self.grid_searches[k].cv_results_[key]        \n",
    "                scores.append(r.reshape(len(params),1))\n",
    "\n",
    "            all_scores = np.hstack(scores)\n",
    "            for p, s in zip(params,all_scores):\n",
    "                rows.append((row(k, s, p)))\n",
    "\n",
    "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
    "\n",
    "        columns = ['estimator', 'min_score', 'mean_score', 'max_score', 'std_score']\n",
    "        columns = columns + [c for c in df.columns if c not in columns]\n",
    "\n",
    "        return df[columns]\n",
    "\n",
    "\n",
    "models1 = {\n",
    "    'ExtraTreesClassifier': ExtraTreesClassifier(),\n",
    "    'RandomForestClassifier': RandomForestClassifier(),\n",
    "    'SVC': SVC(),\n",
    "    'LogisticRegressionClassifier': LogisticRegression(),\n",
    "    'KNeighborsClassifier': KNeighborsClassifier(),\n",
    "    'GradientBoostingClassifier': GradientBoostingClassifier(),\n",
    "    'DecisionTreeClassifier': DecisionTreeClassifier(),\n",
    "}\n",
    "    \n",
    "params1 = {\n",
    "    'ExtraTreesClassifier': { 'n_estimators': [16, 32, 64, 100], 'criterion':['gini','entropy'], 'max_depth': np.arange(3, 15)}, #96\n",
    "    'RandomForestClassifier': { 'n_estimators': [16, 32, 64, 100], 'max_depth':np.arange(3, 15) }, #48\n",
    "    'SVC': [\n",
    "        {'kernel': ['linear'], 'C': [0.025, .1, 1, 10]},\n",
    "        {'kernel': ['rbf'], 'C': [1, 10], 'gamma': [2, 1, .1, .01, 0.001, 0.0001]},\n",
    "    ], \n",
    "    'LogisticRegressionClassifier': { 'solver':['newton-cg', 'lbfgs', 'sag', 'saga'],'fit_intercept':[True, False], 'C':[.5, 1.5, 3, 4, 3.5, 4.5, 5, 10], 'tol': [1e-20, 1e-10, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4]},\n",
    "    'KNeighborsClassifier': {'n_neighbors':[3,5,11,19], 'weights':['uniform', 'distance'], 'metric':['manhattan']},\n",
    "    'DecisionTreeClassifier': {'max_depth':np.arange(3, 15)},\n",
    "    'GradientBoostingClassifier': {},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfarray = df.values\n",
    "X = dfarray[:,1:-1]\n",
    "Y = dfarray[:,-1]\n",
    "\n",
    "#scale values\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "helper1 = EstimatorSelectionHelper(models1, params1)\n",
    "helper1.fit(X_scaled, Y, cv=LeaveOneOut(), n_jobs=-1)\n",
    "helper1.score_summary(sort_by='mean_score')\n",
    "\n",
    "## repeat the above with different sets of features found above ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leave one out cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# depending on the scenario, may need to mask the group labels to be 0s/1s \n",
    "# e.g., for predicting control vs aMCI and AD, I masked controls to be 0 and aMCI/AD to be 1:\n",
    "df['GROUP'].mask(features['group'] == 1, 0, inplace=True)\n",
    "df['GROUP'].mask(features['group'] == 2, 1, inplace=True)\n",
    "df['GROUP'].mask(features['group'] == 3, 1, inplace=True)\n",
    "'''\n",
    "\n",
    "# an explicit way to do leave one out cross validation:\n",
    "\n",
    "dfarray = df.values \n",
    "X = dfarray[:,1:-1] # can narrow it down to smaller sets of features here -> \n",
    "                    # i.e., X = df['feature1', 'feature2','feature3'].values\n",
    "Y = dfarray[:,-1]\n",
    "\n",
    "#scale values\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "#train on all but 1, test on 1\n",
    "\n",
    "predictions = {}\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for row_index in range(len(X)):\n",
    "    \n",
    "    train_x = np.delete(X, row_index, 0)\n",
    "    train_y = np.delete(Y, row_index, 0)\n",
    "    \n",
    "    test_x = X[row_index]\n",
    "    test_y = Y[row_index]\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    Xtrain_scaled = scaler.fit_transform(train_x)\n",
    "    Xtest_scaled = scaler.transform(test_x.reshape(1, -1))\n",
    "    \n",
    "    # depending on the classification sccenario, the most appropriate model changes\n",
    "    rfc = ExtraTreesClassifier(bootstrap= True, class_weight='balanced', criterion= 'gini', max_depth= 11, n_estimators= 100)\n",
    "    \n",
    "    rfc.fit(Xtrain_scaled, train_y) \n",
    "    \n",
    "    prediction = rfc.predict(Xtest_scaled)[0]\n",
    "    \n",
    "    y_true.append(test_y)\n",
    "    y_pred.append(prediction)\n",
    "    \n",
    "    predictions[row_index] = prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from here we can compare predictions to Y:\n",
    "\n",
    "classification_report(y_true, y_pred, target_names=['healthy control', 'aMCI', 'AD'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_true, y_pred, labels=[1,2,3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
