{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for animal fluency feature extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data \n",
    "# a file called \"animal_fluency.xlsx\"\n",
    "\n",
    "# COLUMNS: \n",
    "# - FILE (a unique designator), \n",
    "# - ANIMALS (a list of human transcribed animals. repeats are designated with \"(...)\", non-animals were not transcribed)\n",
    "# - GROUP (1 = control, 2 = aMCI, 3 = AD)\n",
    "\n",
    "animal_fluency = pd.read_excel('animal_fluency.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the number of animals including repetitions, \n",
    "# number of animals without repetitions, \n",
    "# and the number of repetitions\n",
    "\n",
    "animal_fluency['participant_num_animals_with_repetitions'] = animal_fluency.apply(lambda row: len(row['ANIMALS'].split()), axis=1)\n",
    "animal_fluency['participant_num_animals_without_repetitions'] = animal_fluency.apply(lambda row: len(list(filter(lambda x: x[0] != '(', row['ANIMALS'].split()))), axis=1)\n",
    "animal_fluency['participant_num_repetitions'] = animal_fluency.apply(lambda row: int(row['ANIMALS'].count('(')), axis=1)\n",
    "                                                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for computing categories of animals based on cosine distance between consecutive animals\n",
    "\n",
    "# for example, we can make a column for w2v cosines:\n",
    "# first load the pretrained word2vec semantic model, from here: https://code.google.com/archive/p/word2vec/\n",
    "w2vmodel = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)\n",
    "cosines = []\n",
    "for animal_list in animal_fluency['ANIMALS']:\n",
    "    this_cosine_list = []\n",
    "    for i in range(len(animal_list) - 1):\n",
    "        this_cosine_list.append(w2vmodel.similarity(animal_list[i], animal_list[i+1]))\n",
    "    cosines.append(this_cosine_list)\n",
    "animalfluency['w2v_cosines'] = cosines\n",
    "\n",
    "\n",
    "# thresholded function splits snimals into appropriate categories given a threshold\n",
    "# if the cosine falls below the threshold, a new category is started\n",
    "# requires a column called cosines in the dataframe which is a list of cosine values between consecutive animals\n",
    "# can be computed with any embeddings\n",
    "\n",
    "def thresholded(threshold, c):\n",
    "    threshold_groups_X = []\n",
    "    for a, c in zip(animal_fluency.ANIMALS, animal_fluency[c]):\n",
    "        \n",
    "        cosines = ast.literal_eval(c)\n",
    "        cosines.insert(0, 0)\n",
    "        \n",
    "        this_group = []\n",
    "        this_participant = []\n",
    "        \n",
    "        for an, co in zip(a.split(' '), cosines):\n",
    "            # can edit to not take into account repeated animals as well \n",
    "            # by keeping track of whether the \"an\" variable has been seen previously \n",
    "            \n",
    "            if co == 0:\n",
    "                this_group.append(an)\n",
    "            elif co <= threshold:\n",
    "                this_participant.append(this_group)\n",
    "                this_group = [an]\n",
    "            else:\n",
    "                this_group.append(an)\n",
    "\n",
    "        this_participant.append(this_group)\n",
    "        threshold_groups_X.append(this_participant)\n",
    "        \n",
    "    return threshold_groups_X\n",
    "\n",
    "\n",
    "# using the thresholded function to create cosine features\n",
    "# with w2v embeddings \n",
    "participant_num_categories = []\n",
    "participant_avg_items_per_category = []\n",
    "participant_std_items_per_category = []\n",
    "participant_min_items_per_category = []\n",
    "participant_max_items_per_category = []\n",
    "\n",
    "d_vals = {}\n",
    "\n",
    "for k in np.arange(0.8, 0.975, .005):\n",
    "    y = round(k, 3)\n",
    "    v = thresholded(y, 'w2v_cosines')\n",
    "    \n",
    "    new_name_num = str(y) + '_participant_num_categories_w2v'\n",
    "    new_name_avg = str(y) + '_avg_items_per_category_w2v'\n",
    "    new_name_std = str(y) + '_std_items_per_category_w2v'\n",
    "    new_name_min = str(y) + '_min_items_per_category_w2v'\n",
    "    new_name_max = str(y) + '_max_items_per_category_w2v'\n",
    "    new_name_catdivwords = str(y) + '_participant_categories_div_totalwords_w2v'\n",
    "    \n",
    "    d_vals[new_name_num] = []\n",
    "    d_vals[new_name_avg] = []\n",
    "    d_vals[new_name_std] = []\n",
    "    d_vals[new_name_min] = []\n",
    "    d_vals[new_name_max] = []\n",
    "    d_vals[new_name_catdivwords] = []\n",
    "\n",
    "    flat_v = [item for sublist in v for item in sublist]\n",
    "\n",
    "    for this_participant in v:\n",
    "        d_vals[new_name_num].append(len(this_participant))\n",
    "        catstats = []\n",
    "        for cat in this_participant:\n",
    "            catstats.append(len(cat))\n",
    "        d_vals[new_name_avg].append(np.average(catstats))\n",
    "        d_vals[new_name_std].append(np.std(catstats))\n",
    "        d_vals[new_name_min].append(np.amin(catstats))\n",
    "        d_vals[new_name_max].append(np.amax(catstats))\n",
    "        d_vals[new_name_catdivwords].append(float(len(this_participant))/float(len(flat_v)))\n",
    "\n",
    "        \n",
    "# adding thresholding features to the dataframe\n",
    "for k, v in d_vals.items():\n",
    "    animal_fluency[k]=v \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reference, also BERT cosine lists:\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_anim_cosines(a_list):\n",
    "\n",
    "  data_tensors = []\n",
    "\n",
    "  for a in a_list:\n",
    "\n",
    "    # tokenize the text input  \n",
    "    # A [CLS] token is inserted at the beginning of the first sentence and a [SEP] token is inserted at the end of each sentence.\n",
    "    #some words are split so the len of tokenized_text will be AT LEAST len(line) + 2\n",
    "    tokenized_text = tokenizer.encode(a)\n",
    "    \n",
    "    # convert indexed tokens in a PyTorch tensor\n",
    "    input_ids = torch.tensor(tokenized_text).unsqueeze(0)\n",
    "    \n",
    "    # run the input tensor through the BertModel\n",
    "    # see text in above cell for what is contained in outputs variable\n",
    "    outputs = model(input_ids)\n",
    "\n",
    "    # get the last_hidden_state\n",
    "    last_hidden_state = outputs[0]\n",
    "\n",
    "    # last hidden state is dimension (batch_size, sequence_length, hidden_size)\n",
    "    # we have one batch so grab this single batch - this_batch is a tensor for each token in tokenized_text\n",
    "    this_batch = last_hidden_state[0]\n",
    "    \n",
    "    # now get the 768 dimension vector for the CLS token (the first in the list) \n",
    "    cls_vector = this_batch[0]\n",
    "\n",
    "    data_tensors.append(cls_vector)\n",
    "  \n",
    "  cosine_sims = []\n",
    "  i = 0\n",
    "  while i < len(data_tensors)-1:\n",
    "    \n",
    "    # get two animal vectors\n",
    "    anim1 = data_tensors[i]\n",
    "    anim2 = data_tensors[i+1]\n",
    "\n",
    "    cosine_sim = 1 - spatial.distance.cosine(anim1.detach().numpy(), anim2.detach().numpy())\n",
    "    \n",
    "    cosine_sims.append(cosine_sim)\n",
    "\n",
    "    i += 1\n",
    "\n",
    "  return cosine_sims\n",
    "\n",
    "all_BERT_cosines = []\n",
    "for a_list in animal_fluency['ANIMALS']):\n",
    "  anims = a_list.split()\n",
    "  cosines = get_anim_cosines(anims_edited)\n",
    "  all_BERT_cosines.append(cosines)\n",
    "  \n",
    "animal_fluency['BERT_cosines'] = all_BERT_cosines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the (updated) troyer categories\n",
    "\n",
    "troyer_categories = {'Africa': ['flamingo','water-buffalo','wild-dog','aardvark', 'antelope', 'kudu','buffalo', 'camel', 'chameleon', 'cheetah','chimpanzee', 'cobra', 'eland', 'elephant', 'gazelle', 'giraffe', 'gnu', 'gorilla','gorillas','hippopotamus', 'hyena', 'impala', 'jackal', 'lemur', 'leopard', 'lion', 'manatee', 'mongoose', 'monkey', 'ostrich', 'panther', 'rhinoceros', 'rhino','tiger', 'tigers','wildebeest', 'warthog', 'zebra'],\n",
    "                     'Australia': ['koala-bear', 'koala','emu', 'kangaroo', 'kiwi', 'opossum', 'possum','platypus', 'Tasmaniandevil', 'wallaby', 'wombat'],\n",
    "                     'Arctic/Far North': ['kodiak-bear','auk', 'caribou', 'musk ox', 'penguin', 'polar bear','reindeer', 'seal', 'wolverine'],\n",
    "                     'Farm': ['chicken','pony', 'ponies','lamb','chicken', 'cow', 'bull','donkey', 'burrow','ass','ferret', 'goat', 'goats','horse', 'horses','steer','mule','mules', 'pig','boar','hog','sheep', 'ram','turkey'],\n",
    "                     'North America': ['kodiak-bear','panda','bears','badger', 'bear', 'grizzly','beaver', 'bobcat', 'caribou', 'chipmunk','cougar','catamount', 'deer', 'elk', 'fox', 'foxes','moose', 'mountain-lion', 'puma', 'rabbit', 'raccoon','skunk', 'squirrel', 'chipmunk','wolf', 'wolves', 'vole'],\n",
    "                     'Water': ['catfish','zebra-fish', 'puffer-fish', 'seahorse','tadpole','stingray','goldfish','eel','crabs','crab','alligator', 'auk', 'beaver', 'crocodile', 'dolphin', 'porpoise', 'fish', 'frog','bullfrog','lobster', 'manatee', 'muskrat', 'newt', 'octopus', 'otter', 'sea-otter','oyster', 'penguin','platypus', 'salamander', 'sealion','sea-lion', 'seal', 'shark', 'toad', 'turtle', 'whale'],\n",
    "                     'Beasts of burden': ['camel', 'donkey', 'burrow','ass','horses','horse','steer','pony','ponies', 'llama', 'ox', 'oxen','alpaca'],\n",
    "                     'Fur': ['beaver', 'chinchilla', 'fox', 'foxes','mink', 'rabbit', 'alpaca'],\n",
    "                     'Pets': ['fish','bird','ferret','snake','lizard','tarantula','puppies','puppy','kitten','kittens','bulldog','budgie', 'bunny_rabbit', 'canary', 'cat','cats', 'dog','dogs','poodle','gerbil', 'goldfish','golden retriever', 'guinea-pig', 'hamster', 'parrot', 'rabbit'],\n",
    "                     'Bird': ['emu','turkeys','chickadee','albatross','crow','snow-bird','song-bird','cockatoo','black-bird','bat','blackbird','fowl','snowbird','flamingo','ostriches','yellow-finch','peacock','wood-pecker','vulture','pigeon','goose','geese','hawk','sparrow','rooster','duck','swan','mandarin-duck','cardinal','blue-bird','hen','pheasant','ibis','eagles','dove','doves','birds','falcon','owl','owls','bird','budgie', 'condor', 'eagle', 'finch', 'kiwi', 'macaw', 'parrot', 'parakeet','pelican', 'penguin', 'robin', 'toucan', 'woodpecker'],\n",
    "                     'Bovine': ['bison', 'buffalo', 'cow', 'bull','musk ox', 'yak', 'water-buffalo'],\n",
    "                     'Canine': ['coyote', 'dog','dogs', 'fox', 'foxes','hyena', 'jackal', 'wolf'],\n",
    "                     'Deer': ['gazelles','antelope', 'kudu','caribou', 'eland', 'elk', 'gazelle', 'gnu', 'impala', 'moose','reindeer', 'wildebeest', 'deer'],\n",
    "                     'Feline': ['bobcat', 'cat', 'cats','cheetah', 'cougar', 'catamount','jaguar', 'leopard', 'lion', 'lions','lynx','mountain-lion', 'ocelot', 'panther', 'puma', 'tiger', 'tigers'],\n",
    "                     'Fish': ['starfish','catfish','zebra-fish', 'puffer-fish','tadpole','bass', 'guppy', 'salmon', 'trout', 'goldfish'],\n",
    "                     'Insect': ['tarantula','fly','flies','butterflies','bumblebee','butterfly','spider','insect','cricket','wasp','bee','ant', 'beetle', 'cockroach', 'flea', 'fly', 'praying mantis'],\n",
    "                     'Insectivores': ['aardvark', 'anteater', 'armadillo','hedgehog', 'mole', 'shrew'],\n",
    "                     'Primate': ['monkies','ape', 'baboon', 'chimpanzee', 'gibbon', 'gorilla', 'gorillas','human','people','lemur','sloth', 'marmoset', 'monkey','monkeys','chimpanzees', 'orangutan', 'shrew'],\n",
    "                     'Rabbit': ['coney', 'hare', 'pika', 'rabbit'],\n",
    "                     'Reptile/Amphibian': ['snail','worm','reptile','komodo-dragon','alligator', 'chameleon', 'crocodile', 'frog', 'bullfrog','gecko','iguana', 'lizard', 'newt', 'salamander', 'copperhead','black-snake', 'snake', 'rattlesnake', 'rattle-snake','garden-snake','anaconda','coral-snake','cobra','boa-constrictor','toad', 'tortoise', 'turtle'],\n",
    "                     'Rodent': ['possum','opossum','vole','beaver', 'chinchilla', 'chipmunk', 'gerbil', 'gopher', 'groundhog', 'guinea-pig', 'hamster', 'hedgehog', 'marmot', 'mole', 'mouse', 'mice','muskrat', 'porcupine', 'rat', 'squirrel', 'chipmunk','woodchuck'],\n",
    "                     'Weasel': ['weasel','badger', 'ferret', 'marten', 'mink', 'mongoose', 'otter', 'sea-otter', 'polecat','skunk'],\n",
    "                     'Dinosaur': ['dinosaur']}\n",
    "\n",
    "category_lists = []\n",
    "\n",
    "for al in animal_fluency['ANIMALS']:\n",
    "    catlist = []\n",
    "    for animal in al:\n",
    "        # can remove this if statements if \"forbidden\" words should be counted\n",
    "        if '(' not in animal:\n",
    "            \n",
    "            # remove the comma from the animal\n",
    "            if ',' in animal:\n",
    "                animal = animal[:-1]\n",
    "            \n",
    "            categories = []\n",
    "            for k, v in troyer_categories.items():\n",
    "                if animal.strip() in v:\n",
    "                    categories.append(k)\n",
    "                if animal+'s' in v and k not in categories:\n",
    "                    categories.append(k)\n",
    "                if animal[:-1] in v and k not in categories:\n",
    "                    categories.append(k)\n",
    "            \n",
    "            catlist.append(categories)\n",
    "    \n",
    "    category_lists.append(catlist)\n",
    "\n",
    "# NOTE!!! The final troyer categories given by hand here !!!\n",
    "'''\n",
    "THIS IS WHAT IS SHOWN ON EACH ITERATION\n",
    "\n",
    "dog ['Pets', 'Canine']\n",
    "cat ['Pets', 'Feline']\n",
    "fish ['Water', 'Pets']\n",
    "pig ['Farm']\n",
    "bunny_rabbit ['Rabbit', 'Pets']\n",
    "guinea_pig ['Pets']\n",
    "hamster ['Pets', 'Rodent']\n",
    "giraffe ['Africa']\n",
    "hippo ['Africa']\n",
    "monkey ['Africa', 'Primate']\n",
    "gorilla ['Africa', 'Primate']\n",
    "rhinoceros ['Africa']\n",
    "zebra ['Africa']\n",
    "horse ['Farm', 'Beasts of burden']\n",
    "cow ['Farm', 'Bovine']\n",
    "chicken ['Farm']\n",
    "alligator ['Water', 'Reptile/Amphibian']\n",
    "crocodile ['Water', 'Reptile/Amphibian']      \n",
    "cheetah ['Africa', 'Feline']\n",
    "birds ['Pets', 'Bird']\n",
    "squirrels ['North America', 'Rodent']\n",
    "snakes ['Pets', 'Reptile/Amphibian']\n",
    "rodents ['Rodent']\n",
    "skunks ['North America', 'Weasel']\n",
    "boar ['Farm']\n",
    "\n",
    "THIS IS WHAT IS TYPED INTO THE INPUT:\n",
    "[dog cat fish] [pig] [bunny_rabbit guinea_pig hamster] [giraffe hippo monkey gorilla rhinoceros zebra] [horse cow chicken] [alligator crocodile] [cheetah] [birds] [squirrels] [snakes] [rodents skunks] [boar]\n",
    "'''\n",
    "\n",
    "for al, cl in zip(animal_fluency['ANIMALS'], category_lists):\n",
    "    for a, c in zip(al.split(), cl.split()):\n",
    "        print(a, c)\n",
    "    categorized = input()\n",
    "    cat_list.append(categorized)\n",
    "\n",
    "participant_num_categories = []\n",
    "participant_avg_items_per_category = []\n",
    "participant_std_items_per_category = []\n",
    "participant_min_items_per_category = []\n",
    "participant_max_items_per_category = []\n",
    "\n",
    "for cat in cat_list:\n",
    "    \n",
    "    try:\n",
    "        participant_num_categories.append(cat.count('['))\n",
    "    except:\n",
    "        participant_num_categories.append(0)\n",
    "        participant_avg_items_per_category.append(0)\n",
    "        participant_std_items_per_category.append(0)\n",
    "        participant_min_items_per_category.append(0)\n",
    "        participant_max_items_per_category.append(0)\n",
    "        continue\n",
    "        \n",
    "    categories = cat.split('] [')\n",
    "    cleaned_categories = [wordlist.replace('[', '').replace(']', '').split() for wordlist in categories]\n",
    "    lengths = []\n",
    "    for c in cleaned_categories:\n",
    "        lengths.append(len(c))\n",
    "    \n",
    "    participant_avg_items_per_category.append(np.array(lengths).mean())\n",
    "    participant_std_items_per_category.append(np.array(lengths).std())\n",
    "    participant_min_items_per_category.append(min(lengths))\n",
    "    participant_max_items_per_category.append(max(lengths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing categories / total animals\n",
    "participant_categories_div_totalwords = []\n",
    "for count, cat in zip(participant_num_animals_without_repetitions, participant_num_categories):\n",
    "    participant_categories_div_totalwords.append(cat/count)\n",
    "    \n",
    "    \n",
    "# calculating pivot words\n",
    "participant_pivot_words = []\n",
    "for count, cat in zip(participant_num_animals_without_repetitions, animal_fluency2.CATEGORIES):\n",
    "    try:\n",
    "        categories = cat.split('] [')\n",
    "    except:\n",
    "        participant_pivot_words.append(0)\n",
    "        continue\n",
    "    cleaned_categories = [wordlist.replace('[', '').replace(']', '').split() for wordlist in categories]\n",
    "    lengths = 0\n",
    "    for c in cleaned_categories:\n",
    "        lengths += len(c)\n",
    "    participant_pivot_words.append(lengths-count)\n",
    "\n",
    "    \n",
    "# adding troyer features to dataframe\n",
    "animal_fluency['participant_num_categories_troyer'] = participant_num_categories\n",
    "animal_fluency['participant_avg_items_per_category_troyer'] = participant_avg_items_per_category\n",
    "animal_fluency['participant_std_items_per_category_troyer'] = participant_std_items_per_category\n",
    "animal_fluency['participant_min_items_per_category_troyer'] = participant_min_items_per_category\n",
    "animal_fluency['participant_max_items_per_category_troyer'] = participant_max_items_per_category\n",
    "animal_fluency['participant_categories_div_totalwords_troyer'] = participant_categories_div_totalwords\n",
    "animal_fluency['participant_pivot_words_troyer'] = participant_pivot_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LSA features (animal vector length, cosines, etc) were computed with direct access to the http://lsa.colorado.edu/ service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_fluency.to_excel('animal_fluency_features.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the f statistics for individual features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in animal_fluency.columns:\n",
    "    if column != 'FILE' and column != 'ANIMALS' and column != 'GROUP' and 'cosines' not in column:\n",
    "        \n",
    "        ones = animal_fluency[animal_fluency.GROUP==1]\n",
    "        twos = animal_fluency[animal_fluency.GROUP==2]\n",
    "        threes = animal_fluency[animal_fluency.GROUP==3]\n",
    "        twosthrees = animal_fluency[(animal_fluency.GROUP==2)|(animal_fluency.GROUP==3)]\n",
    "        \n",
    "        f1, p1 = stats.f_oneway(ones[column].dropna(), twos[column].dropna(), threes[column].dropna())\n",
    "        f2, p2 = stats.f_oneway(ones[column].dropna(), twos[column].dropna())\n",
    "        f3, p3 = stats.f_oneway(ones[column].dropna(), threes[column].dropna())\n",
    "        f4, p4 = stats.f_oneway(twos[column].dropna(), threes[column].dropna())\n",
    "        f5, p5 = stats.f_oneway(ones[column].dropna(), twosthrees[column].dropna())\n",
    "\n",
    "        print(column, 'overall', (f1,p1), '1vs2', (f2,p2),'1vs3', (f3,p3),'2vs3', (f4,p4), '1vs23', (f5,p5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
